{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_imdb_binary_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPt/uI0YpE/vWFtpOJR80wy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpcompartir/dl_notebooks/blob/main/keras_imdb_binary_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "U60KNEpZb32a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be classifying the IMDB dataset (dejavu)according to whether the sentiment of each review is positive or negative - hence binary classification!"
      ],
      "metadata": {
        "id": "YZdAZ5uJcYWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000) #Keeping the top 10k most frequent words of the dataset's vocab. As opposed to the ~88.5k unique words in total (keeping the size of our data down)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hwOlqyfcQRu",
        "outputId": "ca3a4881-431b-4810-bee2-35b4d7d6359a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the data we can see that each post is a series of integers - the text has come ready tokenised."
      ],
      "metadata": {
        "id": "trnxmg2pdSxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0], train_labels[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x60rAXdVcXfs",
        "outputId": "521c70be-5f40-44a7-c28c-adfd7310405e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([1,\n",
              "  14,\n",
              "  22,\n",
              "  16,\n",
              "  43,\n",
              "  530,\n",
              "  973,\n",
              "  1622,\n",
              "  1385,\n",
              "  65,\n",
              "  458,\n",
              "  4468,\n",
              "  66,\n",
              "  3941,\n",
              "  4,\n",
              "  173,\n",
              "  36,\n",
              "  256,\n",
              "  5,\n",
              "  25,\n",
              "  100,\n",
              "  43,\n",
              "  838,\n",
              "  112,\n",
              "  50,\n",
              "  670,\n",
              "  2,\n",
              "  9,\n",
              "  35,\n",
              "  480,\n",
              "  284,\n",
              "  5,\n",
              "  150,\n",
              "  4,\n",
              "  172,\n",
              "  112,\n",
              "  167,\n",
              "  2,\n",
              "  336,\n",
              "  385,\n",
              "  39,\n",
              "  4,\n",
              "  172,\n",
              "  4536,\n",
              "  1111,\n",
              "  17,\n",
              "  546,\n",
              "  38,\n",
              "  13,\n",
              "  447,\n",
              "  4,\n",
              "  192,\n",
              "  50,\n",
              "  16,\n",
              "  6,\n",
              "  147,\n",
              "  2025,\n",
              "  19,\n",
              "  14,\n",
              "  22,\n",
              "  4,\n",
              "  1920,\n",
              "  4613,\n",
              "  469,\n",
              "  4,\n",
              "  22,\n",
              "  71,\n",
              "  87,\n",
              "  12,\n",
              "  16,\n",
              "  43,\n",
              "  530,\n",
              "  38,\n",
              "  76,\n",
              "  15,\n",
              "  13,\n",
              "  1247,\n",
              "  4,\n",
              "  22,\n",
              "  17,\n",
              "  515,\n",
              "  17,\n",
              "  12,\n",
              "  16,\n",
              "  626,\n",
              "  18,\n",
              "  2,\n",
              "  5,\n",
              "  62,\n",
              "  386,\n",
              "  12,\n",
              "  8,\n",
              "  316,\n",
              "  8,\n",
              "  106,\n",
              "  5,\n",
              "  4,\n",
              "  2223,\n",
              "  5244,\n",
              "  16,\n",
              "  480,\n",
              "  66,\n",
              "  3785,\n",
              "  33,\n",
              "  4,\n",
              "  130,\n",
              "  12,\n",
              "  16,\n",
              "  38,\n",
              "  619,\n",
              "  5,\n",
              "  25,\n",
              "  124,\n",
              "  51,\n",
              "  36,\n",
              "  135,\n",
              "  48,\n",
              "  25,\n",
              "  1415,\n",
              "  33,\n",
              "  6,\n",
              "  22,\n",
              "  12,\n",
              "  215,\n",
              "  28,\n",
              "  77,\n",
              "  52,\n",
              "  5,\n",
              "  14,\n",
              "  407,\n",
              "  16,\n",
              "  82,\n",
              "  2,\n",
              "  8,\n",
              "  4,\n",
              "  107,\n",
              "  117,\n",
              "  5952,\n",
              "  15,\n",
              "  256,\n",
              "  4,\n",
              "  2,\n",
              "  7,\n",
              "  3766,\n",
              "  5,\n",
              "  723,\n",
              "  36,\n",
              "  71,\n",
              "  43,\n",
              "  530,\n",
              "  476,\n",
              "  26,\n",
              "  400,\n",
              "  317,\n",
              "  46,\n",
              "  7,\n",
              "  4,\n",
              "  2,\n",
              "  1029,\n",
              "  13,\n",
              "  104,\n",
              "  88,\n",
              "  4,\n",
              "  381,\n",
              "  15,\n",
              "  297,\n",
              "  98,\n",
              "  32,\n",
              "  2071,\n",
              "  56,\n",
              "  26,\n",
              "  141,\n",
              "  6,\n",
              "  194,\n",
              "  7486,\n",
              "  18,\n",
              "  4,\n",
              "  226,\n",
              "  22,\n",
              "  21,\n",
              "  134,\n",
              "  476,\n",
              "  26,\n",
              "  480,\n",
              "  5,\n",
              "  144,\n",
              "  30,\n",
              "  5535,\n",
              "  18,\n",
              "  51,\n",
              "  36,\n",
              "  28,\n",
              "  224,\n",
              "  92,\n",
              "  25,\n",
              "  104,\n",
              "  4,\n",
              "  226,\n",
              "  65,\n",
              "  16,\n",
              "  38,\n",
              "  1334,\n",
              "  88,\n",
              "  12,\n",
              "  16,\n",
              "  283,\n",
              "  5,\n",
              "  16,\n",
              "  4472,\n",
              "  113,\n",
              "  103,\n",
              "  32,\n",
              "  15,\n",
              "  16,\n",
              "  5345,\n",
              "  19,\n",
              "  178,\n",
              "  32],\n",
              " 1)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can go from integers back to original tokens and original text(minus the lower frequency words which have been clipped from the dataset via the num_words = 10000 argument). Indices are off by 3 (i - 3) because tokens 0,1, and 2 are reserved indices for padding, start of sequence & unknown respectively."
      ],
      "metadata": {
        "id": "cO72XwChdawG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "decoded_review = \" \".join([reverse_word_index.get(i - 3, \"?\") for i in train_data[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfvs5bJGdXvI",
        "outputId": "50cdcdaf-f1a2-45d1-b2f7-4413ebc1b586"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a quick function to do this for any review within the data, setting train_data as the default arg for data in case wanting to call the function many times etc."
      ],
      "metadata": {
        "id": "CLF7brUHgaBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_review(index, data = train_data):\n",
        "  word_index = imdb.get_word_index()\n",
        "  reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "  \n",
        "  return \" \".join([reverse_word_index.get(i - 3, \"?\") for i in data[index]])\n"
      ],
      "metadata": {
        "id": "7_Nmxjqde426"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decode_review(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "dNzzttehfeVR",
        "outputId": "c9069983-394d-429c-cf25-e8909df5860f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"? this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had ? working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how ? this is to watch save yourself an hour a bit of your life\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data[0]), len(train_data[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8riD8Fngm7u",
        "outputId": "d9273168-209e-40b6-a719-c3ef4a28f3e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(218, 189)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We notice that the length of each list (each document is now a list of integers) differs, and we recall that neural networks (and more generally matrix multiplication) require inputs to be of a certain shape - in the case of neural networks we need tensors of precisely the same shape, so we need to either pad, or truncate our sequences so that they are of the same length. \n",
        "\n",
        "---\n",
        "We could either condense our lists (or vectors) into dense word embeddings, or sparsely encode with one-hot vectors i.e. if our document contains one word, which is tokenised to the integer 5, put a one in the 5th index position and put a 0 in the other 9999 index positions. For now we will one-hot encode each document by defining a vectorise_sequence function, which instatiates a tensor of the appropriate shape, then enumerates each list (gives each element an index position), maps over each index, and sequence in the enumerated list, then maps over each integer (j) in each sequence, and sets the ith jth index of the tensor as 1.\n"
      ],
      "metadata": {
        "id": "oh9jhr1SglZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorise_sequences(sequences, dimension = 10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    for j in sequence:\n",
        "      results[i, j] = 1\n",
        "  return results"
      ],
      "metadata": {
        "id": "b1MR3EgihY4C"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = vectorise_sequences(train_data)\n",
        "x_test = vectorise_sequences(test_data)"
      ],
      "metadata": {
        "id": "QiVHVrRZfgrt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.asarray(train_labels).astype(\"float32\")\n",
        "y_test = np.asarray(test_labels).astype(\"float32\")"
      ],
      "metadata": {
        "id": "SFSaRogiiH2b"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Now we have input data in the form of tensors, and input labels as scalars (1, 0) for the binary classification. We instantiate a model with two intermediate layers with 16 units and a relu activation function, followed by a classification head with a sigmoid (logisitic) activation function."
      ],
      "metadata": {
        "id": "VFIgaPMgjCkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "                          layers.Dense(16, activation = \"relu\"),\n",
        "                          layers.Dense(16, activation = \"relu\"),\n",
        "                          layers.Dense(1, activation = \"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "IHFi3zgdiwXv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then compile the model with a loss function, an optimiser and some metrics. As we are attempting binary classification, we'll use an rmsprop optimiser, binary cross entropy loss function & accuracy as a metric (the labels are balances so accuracy is fine.\n",
        "\n"
      ],
      "metadata": {
        "id": "DqDl5TjXjxeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = \"rmsprop\",\n",
        "              loss = \"binary_crossentropy\",\n",
        "              metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "3mg4Z3mfpG11"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(np.count_nonzero(train_labels == 0), np.count_nonzero(train_labels == 1)), (np.count_nonzero(test_labels == 0), np.count_nonzero(test_labels ==1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnDYriILjmng",
        "outputId": "dc0f84dc-3909-4a36-f464-2cb4c38d448a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((12500, 12500), (12500, 12500))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the test & training sets, but we also need a validation set that we validate the final model on."
      ],
      "metadata": {
        "id": "KIgnt59wnbAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "metadata": {
        "id": "sv6N7GPCjm6n"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(partial_x_train, partial_y_train,\n",
        "                    epochs = 20, batch_size = 512,\n",
        "                    validation_data = (x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAr3E1bcoKmC",
        "outputId": "ce7382a2-69c7-4cce-8ba7-448d760ae8ef"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "30/30 [==============================] - 5s 41ms/step - loss: 0.5297 - accuracy: 0.7865 - val_loss: 0.4166 - val_accuracy: 0.8671\n",
            "Epoch 2/20\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.3353 - accuracy: 0.9015 - val_loss: 0.3279 - val_accuracy: 0.8836\n",
            "Epoch 3/20\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.2471 - accuracy: 0.9241 - val_loss: 0.2922 - val_accuracy: 0.8870\n",
            "Epoch 4/20\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.1928 - accuracy: 0.9405 - val_loss: 0.2839 - val_accuracy: 0.8863\n",
            "Epoch 5/20\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.1550 - accuracy: 0.9541 - val_loss: 0.3057 - val_accuracy: 0.8787\n",
            "Epoch 6/20\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.1259 - accuracy: 0.9638 - val_loss: 0.2882 - val_accuracy: 0.8867\n",
            "Epoch 7/20\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.1060 - accuracy: 0.9697 - val_loss: 0.3020 - val_accuracy: 0.8851\n",
            "Epoch 8/20\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0880 - accuracy: 0.9757 - val_loss: 0.3216 - val_accuracy: 0.8809\n",
            "Epoch 9/20\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0728 - accuracy: 0.9801 - val_loss: 0.3512 - val_accuracy: 0.8770\n",
            "Epoch 10/20\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0595 - accuracy: 0.9849 - val_loss: 0.3627 - val_accuracy: 0.8786\n",
            "Epoch 11/20\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0452 - accuracy: 0.9907 - val_loss: 0.4854 - val_accuracy: 0.8572\n",
            "Epoch 12/20\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0383 - accuracy: 0.9926 - val_loss: 0.4871 - val_accuracy: 0.8612\n",
            "Epoch 13/20\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0321 - accuracy: 0.9931 - val_loss: 0.4517 - val_accuracy: 0.8734\n",
            "Epoch 14/20\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.0232 - accuracy: 0.9962 - val_loss: 0.4763 - val_accuracy: 0.8737\n",
            "Epoch 15/20\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0206 - accuracy: 0.9963 - val_loss: 0.5082 - val_accuracy: 0.8717\n",
            "Epoch 16/20\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0151 - accuracy: 0.9975 - val_loss: 0.5369 - val_accuracy: 0.8701\n",
            "Epoch 17/20\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0108 - accuracy: 0.9991 - val_loss: 0.5706 - val_accuracy: 0.8704\n",
            "Epoch 18/20\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0099 - accuracy: 0.9988 - val_loss: 0.6086 - val_accuracy: 0.8693\n",
            "Epoch 19/20\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0093 - accuracy: 0.9981 - val_loss: 0.6705 - val_accuracy: 0.8644\n",
            "Epoch 20/20\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0044 - accuracy: 0.9998 - val_loss: 0.6630 - val_accuracy: 0.8680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation accuracy @ 88.7% in best model version. Can see the overtraining effect as the val_loss keeps increasing after about epoch ~ 10. But let's plot the outcome. We saved our results as history."
      ],
      "metadata": {
        "id": "S2LAmEhPpd5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez0UnHtVoMQt",
        "outputId": "8750674c-3258-4de4-c6e0-ae3b8b4cba83"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "10-y-duTqmKf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
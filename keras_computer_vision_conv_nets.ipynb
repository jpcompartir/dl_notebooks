{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNZmfF3gYx6OdYKMAmAQwnQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpcompartir/dl_notebooks/blob/main/keras_computer_vision_conv_nets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "We'll look at simple convolutional neural networks (convnets) followed by more compelx convnet architectures and advanced techniques. We use conv nets for a variety of tasks in computer vision\n",
        "* image classification\n",
        "* object detection\n",
        "* image segmentation\n",
        "\n",
        "To use convnets we first need to pre-process images into the dimensions each architecture accepts as input, generally we represent each image as a matrix of pixels with rgb values. For example an image of a dog could be represented as a an x by x matrix (2d), and a region of the x by x matrix will contain the eyes, ears, paws etc. A convnet requires inputs of shape image height, image width, and number of channels. break down the image into its constituent parts with increasing levels of specificity, i.e. the first layers of the convent will most resemble the original image and the later layers will least. In the early layers we find outlines of whole animals/structures in later layers we will find increasingly specific 'features' like outline of eyebrows, nostrils etc.\n",
        "\n",
        "One of the great strengths of a convent is that the network learns these features in a context-independent manner, i.e. it can recognise features in similar-but-distinct images. Which if we think about the task of computer vision, is very important - we need the network to recognise the features whether a small thing in the images changes or not.\n",
        "\n",
        "As we go we will get into pre-processing, convolutional layers, pooling layers, strides, data augmentation, regularisation, residual connections to avoid vanishing gradients in deep networks, depthwise separable convolutions and transfer learning. We begin by following Chollet's introduction to convents.\n",
        "\n",
        "A basic convnet is a block of 2d convolutional layers and maxpooling2d layers.\n"
      ],
      "metadata": {
        "id": "5Mp3WTFxyx86"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdsoyIm-ytvZ",
        "outputId": "2321636e-4085-47c3-f9dd-046dc220a0b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 13, 13, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 3, 3, 128)         73856     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1152)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                11530     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 104,202\n",
            "Trainable params: 104,202\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape = (28, 28, 1))\n",
        "x = layers.Conv2D(filters = 32, kernel_size = 3, activation = \"relu\")(inputs)\n",
        "x = layers.MaxPooling2D(pool_size = 2)(x)\n",
        "x = layers.Conv2D(filters = 64, kernel_size = 3, activation = \"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size = 2)(x)\n",
        "x = layers.Conv2D(filters = 128, kernel_size = 3, activation = \"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "\n",
        "outputs = layers.Dense(10,  activation = \"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs = inputs, outputs = outputs)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's think about what happened:\n",
        "We defined the dimensions of our inputs, they are image_height = 28, image_width = 28, channels = 1. Channels = 1 means that we are looking at a gray image - the image has 1 channel which is levels of gray. Channels = 3 would refer to RGB - 3 separate channels, 1 for each colour, or depth 3.\n",
        "Then we feed our inputs inot a Conv2d layer, with 32 filters, a kernel size of 3 and a relu activation - kernel_size 3 means that we will be looking at 3x3 blocks of our image and the relu activation we should be comfortable with by now, but recall that it returns 0 for any input <= 0 and input for any input > 0 i.e. if the input is positive the value is returned if the value is zero or negative then 0 is returned. We can think of the transformation as input * 0 if input <= 0 or input * 1 if input >  0.\n",
        "The input is then fed into a MaxPooling2d layer, and the pool_size = 2 - so we make sliding widows across our kernal of 2, often with stride 2(skip 1 square) and we take the max value for the 2 squares - think of this as downsampling our input by a factor of 2 (if stride =2).\n",
        "Then we feed the pooled inputs into a new Conv2D layer which as 2x as many filters as our previous layer and repeat until the flatten and classification layers.\n",
        "\n",
        "When we take a look at the model summary we notice the change in output for each level and the increasing number of parameters until we flatten the outputs - we flatten because our classification head accepts a vector as input. We use a sotfmax activation to output a probability distribution.\n",
        "\n",
        "Now we need to get some data and compile our model with an optimizer, a loss function and some metrics before we fit our model to our data."
      ],
      "metadata": {
        "id": "tOJmLV8dy7BJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape(60000, 28, 28, 1)\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "\n",
        "model.compile(optimizer = \"rmsprop\",\n",
        "             loss = \"sparse_categorical_crossentropy\",\n",
        "             metrics = [\"accuracy\"])\n",
        "\n",
        "model.fit(train_images, train_labels,  epochs = 5, batch_size = 64)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXLjc_GIy18J",
        "outputId": "170ac0da-b171-49d4-9f4c-ba704e61e8c6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "938/938 [==============================] - 54s 57ms/step - loss: 0.1558 - accuracy: 0.9521\n",
            "Epoch 2/5\n",
            "938/938 [==============================] - 53s 57ms/step - loss: 0.0441 - accuracy: 0.9868\n",
            "Epoch 3/5\n",
            "938/938 [==============================] - 53s 56ms/step - loss: 0.0303 - accuracy: 0.9909\n",
            "Epoch 4/5\n",
            "938/938 [==============================] - 53s 56ms/step - loss: 0.0230 - accuracy: 0.9930\n",
            "Epoch 5/5\n",
            "938/938 [==============================] - 54s 57ms/step - loss: 0.0185 - accuracy: 0.9946\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0289 - accuracy: 0.9913\n",
            "Test accuracy: 0.991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We referenced before that convnets learn representations that are context-independent, another way of saying this is to say their learned representations are translation-invariant. We also said that their representations become increasingly more specific the deeper the network becomes - we can think of the layers as hierarchichal - or more specifically spatially hierarchical - i.e. the general representations of the first layer are broken down in the second layer to more specific representations, which are then broken down in the third layer to still more specific representations.\n",
        "\n",
        "The representations of convnets are found in the 'filters' - a parameter we feed into each convent layer to determine the number of representations the layer should output. See page ~205+ for a deeper & visual explanation of conv net architecture including information on kernels, feature maps and padding/border effects.\n",
        "\n",
        "We use padding/border effects to increase the number of x by x windows the network can create features from. Padding takes 2 arguments - \"same\" and \"valid\", where \"valid\" will add no padding, \"same\" - pad such that the output is the same shape and width of the input. See 208 for visual explanation of why padding is important - but imagine a 5 x 5 feature map, there are only 9 tiles which a 3 x 3 window can be centered around."
      ],
      "metadata": {
        "id": "hSnVqldrzzZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the dogs vs cats data set in Colab (doesn't come with Keras)\n",
        "!kaggle competitions download -c dogs-vs-cats\n",
        "#Need to authenticate Kaggle for this, so follow page 212 to set that up."
      ],
      "metadata": {
        "id": "0oHZ7B-zy15x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the dataset is downloaded etc. we take a sampls - 2 x 1k for train, 2 x 500 for validation, 2 x 1k for test - out of 25k original images, to make the problem more realistic (it's hard to get so much data in reality). (2 x - one for cat one for dog)."
      ],
      "metadata": {
        "id": "ZvG2tqJy0ezw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, pathlib\n",
        "\n",
        "original_dir = pathlib.Path(\"dir_to_data\")\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n",
        "\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "    for category in(\"cat\", \"dog\"):\n",
        "        dir = new_base_dir / subset_name / category\n",
        "        os.makedirs(dir)\n",
        "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
        "        for fname in fnames:\n",
        "            shutil.copyfile(src=original_dir / fname, \n",
        "                            dat=dir / fname)\n",
        "            \n",
        "make_subset(\"train\", start_index = 0, end_index = 1000)\n",
        "make_subset(\"validation\", start_index = 1000, end_index = 1500)\n",
        "make_subset('test', start_index = 1500, end_index = 2500)"
      ],
      "metadata": {
        "id": "pfC_eI40y13c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to make a model similarly to how we did before, however this time the images will also be rescaled from 0 - 255 to 0 -1"
      ],
      "metadata": {
        "id": "DHSfEuiQ0jab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape = (180, 180, 3))\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "X = layers.Conv2D(filters = 32, kernel_size = 3, activation = \"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size = 2)(x)\n",
        "x = layers.Conv2D(filter = 64, kernel_size = 3, activation = \"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size = 2)(x)\n",
        "x = layers.Conv2D(filter = 128, kernel_size = 3, activation = \"relu\")(x)\n",
        "x = layers.MaxPooling2D(Pool_size = 2)(x)\n",
        "x = layers.Conv2D(filter = 256, kernel_size = 3, activation = \"relu\")(x)\n",
        "x = layers.MaxPooling2D(Pool_size = 2)(x)\n",
        "x = layers.Conv2D(filter = 256, kernel_size = 3, activation = \"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "\n",
        "outputs = layers.Dense(1, activation = \"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.summary"
      ],
      "metadata": {
        "id": "WuJD0AEPy11H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = \"binary_crossentropy\",\n",
        "             optimizer = \"rmsprop\",\n",
        "             accuracy = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "rfiXjYmcy1xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our data needs to be pre-processed from the directories we created. So we must -\n",
        "* Read the files\n",
        "* Decode the jpg content to RGB  grids of pixels\n",
        "* Convert to floating-point tensors\n",
        "* Resize images to 180 x 180\n",
        "* Batch them up - in this case 32 images per batch\n",
        "\n",
        "We'll use the keras utility functions to carry out these steps:"
      ],
      "metadata": {
        "id": "0FZPk0_l0sj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import image_dataset_from_dictionary\n",
        "\n",
        "train_dataset = image_dataset_from_dictionary(new_base_dir / \"train\",\n",
        "                                             image_size = (180, 180),\n",
        "                                             batch_size = 32)\n",
        "\n",
        "validation_dataset = image_dataset_from_dictionary(new_base_dir / \"validation\", \n",
        "                                                  image_size = (180, 180),\n",
        "                                                  batch_size = 32)\n",
        "\n",
        "test_dataset = image_dataset_from_dictionary(new_base_dir / \"test\",\n",
        "                                            image_size = (180, 180),\n",
        "                                            batch_size = 32)\n",
        "\n",
        "#Can test the output of our datasets (TensorFlow Dataset objects - great for straming data and dealing with big data)\n",
        "for data_batch, labels_batch in train_dataset:\n",
        "    print(\"data_batch_shape: \", data_batch.shape)\n",
        "    print(\"labels_shape: \", labels_batch.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "2xo85uomy1tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally now we can fit the data and monitor how well it converges:"
      ],
      "metadata": {
        "id": "zr3LFYlK0yrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "    filepath = \"convenet_from_scratch.keras\",\n",
        "    save_best_only=True,\n",
        "    monitor = \"val_loss\")\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "train_dataset,\n",
        "epocs = 30,\n",
        "validation_data = validation_dataset,\n",
        "callbacks = callbacks)"
      ],
      "metadata": {
        "id": "CZpqqJql0vA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y5KrQKXy0vLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IEc1Y8tM0vNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZTnACwiR0vPu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
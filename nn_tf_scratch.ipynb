{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn_tf_scratch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPqCw1jBEfMhlgRCVahSrn0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpcompartir/dl_notebooks/blob/main/nn_tf_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9DwQ58p7iiUP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorFlow is itself an API, so claiming to implement a nn from scratch using tf is fairly disingenuous, there are thousands of lines of code which have been abstracted away from us; but let's give ourselves some credit and accept tha the knowledge which we have recently accumulated does give us the privilege of being able to implement a dense NN when before we couldn't!\n"
      ],
      "metadata": {
        "id": "2o98ud7ii6-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We implement a dense neural network, which comprises the Weight matrix dimensions, the output dimensions, and the activation function. We initialise our network random weights.\n",
        "\n",
        "> Indented block\n",
        "\n"
      ],
      "metadata": {
        "id": "1CBF1uKamISJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveDense:\n",
        "  def __init__(self, input_size, output_size, activation):\n",
        "    self.activation = activation\n",
        "\n",
        "    w_shape = (input_size, output_size)\n",
        "    w_initial_value = tf.random.uniform(w_shape,minval = 0, maxval = 1e-1)\n",
        "    self.W = tf.Variable(w_initial_value)\n",
        "\n",
        "    b_shape = (output_size)\n",
        "    b_initial_value = tf.zeros(b_shape)\n",
        "    self.b = tf.Variable(b_initial_value)\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    return self.activation(tf.matmul(inputs, self.W) + self.b)\n",
        "\n",
        "  @property\n",
        "  def weights(self):\n",
        "    return(self.W, self.b)\n",
        "    "
      ],
      "metadata": {
        "id": "1I1pjAaWjcvr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveSequential:\n",
        "  def __init__(self, layers):\n",
        "    self.layers = layers\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    x = inputs\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\n",
        "  @property\n",
        "  def weights(self):\n",
        "    weights = []\n",
        "    for layer in self.layers:\n",
        "      weights += layer.weights\n",
        "    return weights\n"
      ],
      "metadata": {
        "id": "PRjOC_m2kdZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NaiveSequential([\n",
        "    NaiveDense(input_size = 28 * 28, output_size = 512, activation = tf.nn.relu),\n",
        "    NaiveDense(input_size = 512, output_size = 10, activation = tf.nn.softmax)\n",
        "])\n",
        "assert len(model.weights) == 4"
      ],
      "metadata": {
        "id": "HzLj1P6SnA6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "WjqGdqmwoD1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to feed our data in in batches, because NN can be very memory-heavy\n"
      ],
      "metadata": {
        "id": "4K98YpAWvDkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchGenerator:\n",
        "  def __init__(self, images, labels, batch_size = 128):\n",
        "    assert len(images) == len(labels)\n",
        "    self.index = 0\n",
        "    self.images = images\n",
        "    self.labels = labels\n",
        "    self.batch_size = batch_size\n",
        "    self.num_batches = math.ceil(len(images)/ batch_size)\n",
        "  def next(self):\n",
        "    images = self.images[self.index : self.index + self.batch_size]\n",
        "    labels = self.labels[self.index : self.index + self.batch_size]\n",
        "    self.index += self.batch_size\n",
        "    return images, labels"
      ],
      "metadata": {
        "id": "1tPXstHGpJzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll need to implement the training process for one observation, followed by all observations. We include GradientTape so that we can trace the gradient of the individual parts of our NN. "
      ],
      "metadata": {
        "id": "y55ErGBevJH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_training_step(model, images_batch, labels_batch):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images_batch)\n",
        "    per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
        "        labels_batch, predictions)\n",
        "    average_loss = tf.reduce_mean(per_sample_losses)\n",
        "  gradients = tape.gradient(average_loss, model.weights)\n",
        "  update_weights(gradients, model.weights)\n",
        "  return average_loss\n",
        "\n",
        "learning_rate = 1e-3"
      ],
      "metadata": {
        "id": "tZGMW2BQptvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_weights(gradients, weights):\n",
        "  for g, w in zip(gradients, weights):\n",
        "    w.assign_sub(g * learning_rate)"
      ],
      "metadata": {
        "id": "4surtOijwI0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "optimizer = optimizers.SGD(learning_rate = 1e-3)\n",
        "\n",
        "def update_weights(gradients, weights):\n",
        "  optimizer.apply_gradients(zip(gradients, weights))"
      ],
      "metadata": {
        "id": "LS8kM5CtwmwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model, images, labels, epochs, batch_size = 128):\n",
        "  for epoch_counter in range(epochs):\n",
        "    print(f\"Epoch {epoch_counter}\")\n",
        "  batch_generator = BatchGenerator(images, labels)\n",
        "  for batch_counter in range(batch_generator.num_batches):\n",
        "    images_batch, labels_batch = batch_generator.next()\n",
        "    loss = one_training_step(model, images_batch, labels_batch)\n",
        "    if batch_counter % 100 == 0:\n",
        "      print(f\"loss at batch {batch_counter}: {loss: .2f}\")"
      ],
      "metadata": {
        "id": "8AC8qv9Sw4de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist"
      ],
      "metadata": {
        "id": "R-_s3ANsxB93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "id": "bJisS5HzyT6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_images.reshape(60000, 28 * 28)\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "\n",
        "test_images = test_images.reshape(10000, 28 * 28 )\n",
        "test_images = test_images.astype(\"float32\") / 255"
      ],
      "metadata": {
        "id": "A2qSolYrygOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit(model, train_images, train_labels, epochs = 10, batch_size = 128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfA4MQk00Dyb",
        "outputId": "447cea5a-7151-4488-dc3b-d380e45c3121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n",
            "Epoch 6\n",
            "Epoch 7\n",
            "Epoch 8\n",
            "Epoch 9\n",
            "loss at batch 0:  3.94\n",
            "loss at batch 100:  2.21\n",
            "loss at batch 200:  2.21\n",
            "loss at batch 300:  2.06\n",
            "loss at batch 400:  2.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model(test_images)\n",
        "predictions = predictions.numpy()\n",
        "\n",
        "predicted_labels = np.argmax(predictions, axis = 1)\n",
        "matches = predicted_labels == test_labels\n",
        "print(f\"accuracy: {matches.mean():.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h61TpPZgyl0c",
        "outputId": "3cb33424-40f8-4da3-e45a-c9b824c7ad6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "E4cc_etLzGQq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}